<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B1P66THSNQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-B1P66THSNQ');
</script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</h1>
          <div class="is-size-4 publication-authors">
            SIGGRAPH Asia 2025
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yxmu.foo/">Yuxuan Mu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hungyuling.com">Hung Yu Ling</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yi-shi94.github.io">Yi Shi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.dk/citations?user=C-7QfjgAAAAJ&hl=da">Ismael Baira Ojeda</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/perryxi/home">Pengcheng Xi</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://nrc.canada.ca/en/corporate/contact-us/nrc-directory-science-professionals/chang-shu">Chang Shu</a><sup>3</sup>,</span>
              <span class="author-block">
              <a href="https://dl.acm.org/profile/99659572328">Fabio Zinno</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xbpeng.github.io/">Xue Bin Peng</a><sup>1,4</sup></span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Simon Fraser University</span>&nbsp
            <span class="author-block"><sup>2</sup>Electronic Arts</span>&nbsp
            <span class="author-block"><sup>3</sup>National Research Council Canada</span>&nbsp
            <span class="author-block"><sup>4</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.03154" target="_blank"
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Murrol/StableMotion" target="_blank"
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://youtu.be/XftDYsxZs64" target="_blank"
                class="external-link button is-normal is-rounded">
                <span class="icon">
                  <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <p style="text-align: justify; margin-inline: 5% 5%;"> 
    <!-- <blockquote style="border-left: 4px solid #2e7d32; padding: 10px; background-color: #f0fff0;">
      <p> üßë‚Äçüîß <strong>You don‚Äôt need a clean dataset to train a motion cleanup model.</strong><br>
      StableMotion learns to fix corrupted motions directly from raw mocap data ‚Äî no handcrafted data pairs, no synthetic artifact augmentation.</p>
    </blockquote> -->
    <div style="border-left: 6px solid #2e7d32; padding: 16px 20px;
     margin: 1.5em 0; background-color: #f0fff4;
     border-radius: 8px; font-family: 'Segoe UI', sans-serif;
     box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
  <p style="margin: 0; font-size: 1.05em;">
    <strong>TL;DR ‚Äî üßë‚Äçüîß You don‚Äôt need a clean dataset to train a motion cleanup model.</strong><br>
    <span style="color: #1b5e20;">StableMotion learns to fix corrupted motions directly from raw mocap data ‚Äî no handcrafted data pairs, no synthetic artifact augmentation.</span>
  </p>
</div>
    <br> 
    </p>
        <!-- <center><h2 class="title is-3">Abstract</h2></center> -->
    <div class="hero-body has-text-centered">
        <img src = "./static/images/teaser.png" height="100%"></img><br>

    </div>

    <p style="text-align: justify; margin-inline: 5% 5%;"> 
      Motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. Cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. Previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. Constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. 
      <span style="font-weight: bold;
        background: linear-gradient(90deg, #2e7d32, #66bb6a, #2e7d32);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-size: 200% auto;
        animation: shine 3s linear infinite;">
        In this work, we present StableMotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup.
      </span>
      <style>
      @keyframes shine {
        to { background-position: 200% center; }
      }
      </style>
      The core component of our method is the introduction of motion quality indicators, which can be easily annotated‚Äî through manual labeling or heuristic algorithms‚Äîand enable training of quality-aware motion generation models on raw motion data with <i>mixed</i> quality. At test time, the model can be prompted to generate high-quality motions using the quality indicators. Our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. We demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying StableMotion to SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. The trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. On our benchmark dataset, we further show that cleanup models trained with our method on unpaired corrupted data outperform state-of-the-art methods trained on clean or paired data, while also achieving comparable performance in preserving the content of the original motion clips.
    </p>
    <br>    
  </div>
</section>

<section class="section is-light is-small hero">
  <div style="text-align: center; margin: 20px 0;">
      <iframe width="1280" height="720"
        src="https://www.youtube-nocookie.com/embed/XftDYsxZs64" 
          title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          refererpolicy=‚Äúcross-origin-with-strict-origin
          allowfullscreen>
      </iframe>
      </div>
</section>

<section class="section is-light is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered is-centered">The Method</h2>
    <div class="hero-body has-text-centered is-centered">
      <img src = "./static/images/training_anime_crop.gif" height="100%"></img><br>
    </div>
    <div class="content has-text-justified">
          <p>
    Inspired by state-return trajectory modeling in offine RL, we incorporate a frame-level quality indicator variable (QualVar) similar to state-level reward in RL. Our StableMotion framework utilizes a generate-discriminate approach, where a model is jointly trained to evaluate motion quality and generate motion of varying quality, according to QualVar. 
          </p>
    </div>
    <div class="hero-body has-text-centered is-centered">      
      <video id="video" autoplay muted loop playsinline width="100%">
        <source src="./static/video/inference_anime.mp4" type="video/mp4">
      </video>
    </div>
    <div class="content has-text-justified">
          <p>
    Similar to the practice of prompting text-to-image model with ‚Äúphoto-realistic quality‚Äù, QualVar offers a knob to specify the generation quality. This allows the model to cleanup raw mocap data by first identifying corrupted segments and then inpainting them with high-quality motions.
          </p>
    </div>
</section>

<section class="section is-light is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 is-centered has-text-centered">StableMotion-SoccerMocap</h2>

    <video id="video" autoplay muted loop playsinline width="100%">
        <source src="static/video/soccer_demo.mp4" type="video/mp4">
      </video>
    <div class="content has-text-justified">
    <p>
      We apply StableMotion to train a motion cleanup model on SoccerMocap, a 245-hour raw motion capture dataset captured by a prominent game studio. The resulting model, StableMotion-SoccerMocap, effectively fix motion artifacts in newly captured motions from the same mocap system.
    </p>
    </div>
</section>

<section class="section is-light is-small">
  <div class="hero-body">
    <div class="container is-centered has-text-centered  is-max-desktop">
      <h2 class="title is-3 is-centered has-text-centered">StableMotion-BrokenAMASS</h2>
      <div id="results-carousel" class="carousel results-carousel">

      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass1.mp4"
          type="video/mp4">
        </video>
      </div>
      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass2.mp4"
          type="video/mp4">
      </div>
      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass3.mp4"
          type="video/mp4">
      </div>
      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass4.mp4"
          type="video/mp4">
      </div>
      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass5.mp4"
          type="video/mp4">
      </div>
      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass6.mp4"
          type="video/mp4">
      </div>
      <div class="column is-centered has-text-centered">
                <video poster="" id="tree" autoplay  muted loop playsinline height="100%">
          <source src="static/video/gallery/brokenamass7.mp4"
          type="video/mp4">
      </div>
  </div>
  <div class="content has-text-justified">
          <p>
      We benchmark the performance of StableMotion on a publicly available dataset, AMASS, by introducing synthetic corruption to construct a benchmark dataset, BrokenAMASS. This demonstrates that, with StableMotion, motion cleanup models can be effectively trained even on datasets exhibiting severe motion corruptions.
    </p>
    </div>
</div>
</div>
</section>

<section class="section is-light is-small">
  <div class="container is-max-desktop">
<div style="border-left: 4px solid #eee; padding: 10px 16px; margin: 20px 0; background-color: #fafafa; font-size: 0.9em; color: #555;">
  <h2 class="title is-3 has-text-centered is-centered">Test-Time Techniques</h2>
</div>
    <details>
  <summary>We also propose a number
of test-time techniques that harness the sample diversity and the
dual functionality of the generate-discriminate model to improve
consistency and preservation of the content in the original motion
clips.</summary>

    <div class="hero-body has-text-centered is-centered">      
      <video id="video" autoplay muted loop playsinline width="100%">
        <source src="./static/video/adaptivecleanup_mini.mp4" type="video/mp4">
      </video>
      <div class="content has-text-justified">
          <p>
    <b>Adaptive cleanup</b> uses soft motion quality evaluation and soft motion inpainting. It allows the model to adaptively adjust the modification to each frame based on the severity of the artifacts, enabling it to better preserve the content of the original frames.
          </p>
    </div>
    </div>
    
    <div class="hero-body has-text-centered is-centered">      
      <video id="video" autoplay muted loop playsinline width="100%">
        <source src="./static/video/ensemble_mini.mp4" type="video/mp4">
      </video>
      <div class="content has-text-justified">
          <p>
    <b>Quality-aware ensemble</b> leverages the diversity of diffusion models and the dual functionality of generate-discriminate models. It ensembles diverse candidate motions by selecting the highest-quality motion based on predicted motion quality scores, leading to more consistent and higher-quality results than performing a single pass of the cleanup model.
          </p>
    </div>
    </div>
    
    </details>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{mu2025stablemotion,
  title={StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data},
  author={Mu, Yuxuan and Ling, Hung Yu and Shi, Yi and Ojeda, Ismael Baira and Xi, Pengcheng and Shu, Chang and Zinno, Fabio and Peng, Xue Bin},
  journal={arXiv preprint arXiv:2505.03154},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>

</body>
</html>
